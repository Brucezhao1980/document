######################################################################################################################
在logstash前面添加kafka集群
######################################################################################################################

下载kafka，这次使用的是2.11版本，主要修改2个配置文件，/config/zookeeper.properties   还有 ./config/server.properties

新建文件夹 zookeeperDir 在里面新建文件myid  内容定义为2

启动命令，先启动zookeeper在启动kafka  

./bin/zookeeper-server-start.sh ./config/zookeeper.properties &

./bin/kafka-server-start.sh ./config/server.properties &

nohup ./bin/kafka-manager -Dconfig.file=conf/application.conf &   (启动kafka监控平台)
##########################################################################################################################
[root@elk-1 opt]# cd kafka_2.11-2.2.0/
[root@elk-1 kafka_2.11-2.2.0]# cat config/zookeeper.properties 

dataDir=/opt/kafka_2.11-2.2.0/zookeeperDir
# the port at which the clients will connect
clientPort=2181
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=1024
tickTime=2000
initLimit=20
syncLimit=10
server.2=10.100.1.201:2888:3888
server.3=10.100.1.202:2888:3888
server.4=10.100.1.203:2888:3888
################################################################################################

[root@elk-1 kafka_2.11-2.2.0]# cat config/server.properties 
broker.id = 2

prot = 9092

host.name = 10.100.1.201

log.dir = /var/log/kafka-log

zookeeper.connect = 10.100.1.201:2181,10.100.1.202:2181,10.100.1.203:2181

num.partitions = 16

log.dirs = /var/log/kafka-log

log.retention.hours = 168
###################################################################################################

修改配置文件完成后scp到其他几个点，集群需要修改 myid 还有broker.id和host.name 

使用启动命令逐个启动。
下面是filebeat的配置文件
#############################################################################

[root@nginx filebeat-6.5.2-linux-x86_64]# cat filebeat.yml 
#=========================== Filebeat prospectors =============================
#filebeat.prospectors:

filebeat.inputs:
- type: log 
  paths:
    - /usr/local/nginx/logs/access.log
  fields:
    logtopic: boss          ## logtopic定义的是kafka的topic名字
    project: logstash
    app_name: nginx
    env: test
    node: 192.168.1.21
    multiline.pattern: '^\['
    multiline.negate: false
    multiline.match: after
  scan_frequency: 10s
- type: log
  paths:
    - /usr/local/nginx/logs/cnepay.cc/*.log
  fields:
    logtopic: boss
    project: logstash
    app_name: nginx-other
    env: test
    node: 192.168.1.21
    multiline.pattern: '^\['
    multiline.negate: false
    multiline.match: after
  scan_frequency: 10s
- type: log
  paths:
    - /usr/local/nginx/logs/cnepay.cc/mposp2way.trunk.cnepay.cc.log
  fields:
    logtopic: boss
    project: mposp2way
    app_name: trunk
    env: test
    node: 192.168.1.21
    multiline.pattern: '^\['
    multiline.negate: false
    multiline.match: after
  scan_frequency: 10s


- type: log
  paths:
    - "/var/log/messages"
  fields:
    logtopic: boss
    project: system
    app_name: system
    env: test
    node: 192.168.1.21
  scan_frequency: 10s



filebeat.config.modules:
    #Glob pattern for configuration loading
    path: ${path.config}/modules.d/*.yml

#  output:
#    logstash:
#      hosts: ["10.100.1.201:5044"]
#
# #----------------------------- Logstash output --------------------------------
# # #output.logstash:
# # # The Logstash hosts
# # # hosts: ["localhost:5044"]

#output.logstash:
#   hosts: ["10.100.1.203:5044", "10.100.1.201:5044"]
#   loadbalance: true
#   index: nginx
#setup.kibana:
#  host: "<10.100.1.201:5601>"


output.kafka:
  enabled: true
  hosts: ["10.100.1.201:9092","10.100.1.202:9092","10.100.1.203:9092"]
  topic: '%{[fields.logtopic]}'  ##匹配fileds字段下的logtopic
  partition.hash:
    reachable_only: true
  compression: gzip
  max_message_bytes: 1000000
  required_acks: 1
logging.to_files: true
#########################################################################################################################3
查看是否输出到kafka
bin/kafka-topics.sh --list --zookeeper elk-1:2181,elk-2:2181,elk-3:2181

重新配置logstash集群配置

[root@elk-1 logstash-6.5.4]# cat config/logstash_kafka.yml 
input{
  kafka{
    bootstrap_servers => "10.100.1.201:9092,10.100.1.202:9092,10.100.1.203:9092"
    topics_pattern  => "boss-*"
    consumer_threads => 5
    decorate_events => true
    codec => "json"
    auto_offset_reset => "latest"
    group_id => "logstash1"   ##logstash 集群需相同
  }
}


filter {
        grok {
               match => ["message", "%{TIMESTAMP_ISO8601:logdate}"]
        }
        date {
               match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
               target => ["datetime"]
        }
        if [fields][project] == "mposp2way" {
                grok {
                       match => { "message" => "(?:%{IPV4:x_real_ip}|-) (?:%{IPV4:x_forward_for}|-) %{IPV4:remote_ip} %{NUMBER:remote_port} (?:%{USER:ident}|-) %{NOTSPACE:auth} \[%{DATA:timestamp}\] \"%{DATA:http_user_agent}\" (?:\"(?:%{URI:http_referrer}|-)\"|%{QS:http_referrer}) \"%{NOTSPACE:ssl_protocol}\" \"%{NOTSPACE:ssl_chiper}\" \"%{BASE16NUM:request_id}\" \"%{WORD:http_method}\" \"%{WORD:http_schema}\" \"%{HOSTNAME:hostname}\" \"(?:%{WORD:request_verb} %{NOTSPACE:request} (?:HTTP/%{NUMBER:http_version})?)\" %{WORD:is_https_on} %{NUMBER:limit_rate} \"%{NOTSPACE:request_filename}\" \"%{NOTSPACE:request_uri}\" \"%{DATA:cookie}\" %{NUMBER:request_lenth} \"%{WORD:request_completion}\" %{NUMBER:request_time} \"%{DATA:server_protocol}\" %{NUMBER:reponse_status} \[%{DATA:reps_bgn_time}\] (?:%{NUMBER:content_length}|-) \"%{DATA:content_type}\" %{NUMBER:body_bytes_sent} \"%{HOSTNAME:hostname}\" \"%{NOTSPACE:document_root}\" \"%{NOTSPACE:document_uri}\" (?:%{IPV4:proxy_add_x_forward_for}|-) %{URIHOST:proxy_host} %{NUMBER:proxy_port} \"%{NOTSPACE:realpath_root}\" \"%{NOTSPACE:uri}\" \"%{NOTSPACE:query_string}\" %{URIHOST:upstream_addr} (?:%{DATA:upstream_cache_status}|-) %{NUMBER:upstream_connect_time} %{NUMBER:upstream_header_time} %{NUMBER:upstream_response_length} %{NUMBER:upstream_response_time} %{NUMBER:upstream_status}" }
                }
        }
        geoip {
                source => "message"
                   database => "/opt/logstash-6.5.4/vendor/bundle/jruby/2.3.0/gems/logstash-filter-geoip-5.0.3-java/vendor/GeoLite2-City.mmdb"
        }
        if [fields][project] == "logstash" {
          grok {
                match => { "message" => "%{IPV4:remote_ip} %{NUMBER:remote_port} (?:%{USER:ident}|-) %{NOTSPACE:auth} \[%{DATA:timestamp}\] \"%{DATA:http_user_agent}\" (?:\"(?:%{URI:http_referrer}|-)\"|%{QS:http_referrer}) \"%{NOTSPACE:ssl_protocol}\" \"%{NOTSPACE:ssl_chiper}\" \"%{BASE16NUM:request_id}\" \"%{WORD:http_method}\" \"%{WORD:http_schema}\" \"%{HOSTNAME:hostname}\" \"(?:%{WORD:request_verb} %{NOTSPACE:request} (?:HTTP/%{NUMBER:http_version})?)\" %{WORD:is_https_on} %{NUMBER:limit_rate} \"%{NOTSPACE:request_filename}\" \"%{NOTSPACE:request_uri}\" \"%{DATA:cookie}\" %{NUMBER:request_lenth} \"%{WORD:request_completion}\" %{NUMBER:request_time} \"%{DATA:server_protocol}\" %{NUMBER:reponse_status} \[%{DATA:reps_bgn_time}\] (?:%{NUMBER:content_length}|-) \"%{DATA:content_type}\" %{NUMBER:body_bytes_sent} \"%{HOSTNAME:hostname}\" \"%{NOTSPACE:document_root}\" \"%{NOTSPACE:document_uri}\" (?:%{IPV4:proxy_add_x_forward_for}|-) %{URIHOST:proxy_host} %{NUMBER:proxy_port} \"%{NOTSPACE:realpath_root}\" \"%{NOTSPACE:uri}\" \"%{NOTSPACE:query_string}\" %{URIHOST:upstream_addr} (?:%{DATA:upstream_cache_status}|-) %{NUMBER:upstream_connect_time} %{NUMBER:upstream_header_time} %{NUMBER:upstream_response_length} %{NUMBER:upstream_response_time} %{NUMBER:upstream_status}" }
          }
        }
        geoip {
                source => "message"
                  database => "/opt/logstash-6.5.4/vendor/bundle/jruby/2.3.0/gems/logstash-filter-geoip-5.0.3-java/vendor/GeoLite2-City.mmdb"
        }
}



output {
        elasticsearch {
                        hosts => ["10.100.1.201:9200", "10.100.1.202:9200", "10.100.1.203:9200"]
                        index => ["%{[fields][project]}-%{[fields][app_name]}-%{+YYYY.MM.dd}"] 
                        user => "elastic"
                        password => "XXXXXXXXX"
                      }
}
###############################################################################################################################################

Es查看是否创建索引 字段是否格式化正确

kafka集群参考
https://www.jianshu.com/p/d02e460cc4da
